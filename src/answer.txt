Thank you to all the reviewers for their comments and suggestions.
Here is my response:
Q1：Novelty: The Innovation of the proposed method is limited and author does not summarize the innovativeness of the paper well enough.(R1, R6)
A1: In this article, we conducted experiments and innovations regarding the pre-processing, model design, and post-processing of cell instance segmentation.
In the pre-processing stage, in addition to basic data augmentation techniques such as Flip, Rotate， we utilized the Taichi Physical Engine and mpm18 to simulate the distribution of cells.
In the model design, although the sizes of cells are relatively consistent within a image, they vary in shape and size across different categories and different images. Moreover, the edge textures of the cells are not clear. Therefore, by concatenating information from different stages and using the combination of Conv and Inconv, we can enhance the model's robustness to different categories. As for the choice of attention mechanism, since the transformer divides the image into fixed sizes and most of the cells in the image are relatively small, the long-range dependency of the conventional transformer's self-attention mechanism does not work well. Hence, we adopt the ConvAttention mechanism, which combines the strengths of convolution and attention. Additionally, we use Convolutional Relative Position Encoding, which encodes the positions of different attention heads with different-sized convolutional kernels, to adapt to multi-scale information.
In the post-processing stage, we employ Marker-Controlled Watershed. It differs from Cellpose, which marks all pixels inside the cells' masks, as we have found that some center points with similar features are prone to be connected into one center point set during the dynamic programming of these center points.

Q2: The paper is not clear enough about some of the formulas, for example, when describing the Factorized Attention Mechanism, the authors don't go into detail about how and why there is a difference from the way ordinary attention scores are calculated.(R3)
A2: In ordinary attention, attention scores are calculated using the formula Att(X) = softmax(QK^{T})V. In Factorized Attention, attention scores are calculated using the formula FactorAtt(X) = Q(softmax(K)^{T})V. The classic attention mechanism has an O(N^2) space complexity and an O(N^2 * C) time complexity. On the other hand, FactorAtt has a space complexity of O(NC + CC) and a time complexity of O(NCC). It has been found that the combination of Factorized Attention and convolutional relative position encoding yields the best results. In this article, we use DepthwiseConv as the position encoding method. For more details, please refer to reference [13] in the article.

Q3: This paper also has some problems in English writing.(R3)
A3: Thank you very much for your correction. I will definitely make the necessary changes.

Q4: The article does not investigate or discuss the reasons behind the lack of performance improvement in the semi-supervised setting using the mean teacher model.(R5)
A4: Perhaps because the distribution of the combination of unlabeled images and labeled images is shifted compared with the train/validation dataset’s distribution, the results on the tuning set are not satisfactory. 

Q5: Given that the application of panoramic segmentation networks in medical image instance segmentation has been established as effective, citing "The application of panoramic segmentation network to medical image segmentation'" may complete and add depth to your theoretical framework.
A5: Thank you very much for your correction. I will look for relevant literature and add it to the citation and related work.

